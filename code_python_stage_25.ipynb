{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import time\n",
        "import math\n",
        "import networkx as nx\n",
        "\n",
        "# =============================\n",
        "# Helpers (fonctions utilitaires)\n",
        "# =============================\n",
        "\n",
        "def digit_sum(node_id: str) -> int:\n",
        "    # Calcule la somme des chiffres dans l’ID du nœud\n",
        "    # Exemple : \"4.1\" -> 4+1 = 5\n",
        "    return sum(int(ch) for ch in node_id if ch.isdigit())\n",
        "\n",
        "def depth_of(node_id: str) -> int:\n",
        "    # Retourne la \"profondeur\" du nœud = première partie avant le point\n",
        "    # Exemple : \"4.1\" -> 4 ; \"7\" -> 7\n",
        "    try:\n",
        "        return int(node_id.split('.')[0])\n",
        "    except Exception:\n",
        "        return 0\n",
        "\n",
        "def immediate_prereqs(graph_adj: dict, failed_nodes: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Trouve les prérequis immédiats (1 saut) des matières échouées par l'étudiant.\n",
        "    \"\"\"\n",
        "    s = set()\n",
        "    for fn in failed_nodes:\n",
        "        s.update(graph_adj.get(fn, []))  # Ajoute les prérequis directs\n",
        "    return list(sorted(s))  # Tri pour avoir un ordre stable\n",
        "\n",
        "def build_graph_from_adj(adj: dict) -> nx.DiGraph:\n",
        "    \"\"\"\n",
        "    Construit le graphe orienté prerequisites -> course.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    for node, prereqs in adj.items():\n",
        "        for p in prereqs:\n",
        "            G.add_edge(p, node)\n",
        "    return G\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Métriques de corrélation (comparaison de classements)\n",
        "# =============================\n",
        "\n",
        "def spearman(order_a: list[str], order_b: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Corrélation de Spearman : mesure si les deux classements ont un ordre similaire.\n",
        "    1.0 = ordres identiques, -1.0 = ordres inverses.\n",
        "    \"\"\"\n",
        "    n = len(order_a)\n",
        "    if n < 2:\n",
        "        return 1.0\n",
        "    rank_a = {x: i for i, x in enumerate(order_a)}\n",
        "    rank_b = {x: i for i, x in enumerate(order_b)}\n",
        "    d2_sum = 0\n",
        "    for x in order_a:\n",
        "        d = rank_a[x] - rank_b[x]\n",
        "        d2_sum += d * d\n",
        "    return 1 - 6 * d2_sum / (n * (n*n - 1))\n",
        "\n",
        "def kendall_tau(order_a: list[str], order_b: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Kendall Tau : mesure le nombre de paires concordantes vs discordantes.\n",
        "    1.0 = mêmes paires ordonnées, -1.0 = ordres complètement opposés.\n",
        "    \"\"\"\n",
        "    n = len(order_a)\n",
        "    if n < 2:\n",
        "        return 1.0\n",
        "    pos_a = {x: i for i, x in enumerate(order_a)}\n",
        "    pos_b = {x: i for i, x in enumerate(order_b)}\n",
        "    concordant = 0\n",
        "    discordant = 0\n",
        "    total_pairs = n * (n - 1) // 2\n",
        "    items = order_a\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            xi, xj = items[i], items[j]\n",
        "            sa = pos_a[xi] - pos_a[xj]\n",
        "            sb = pos_b[xi] - pos_b[xj]\n",
        "            if sa * sb > 0:      # paires dans le même ordre\n",
        "                concordant += 1\n",
        "            elif sa * sb < 0:    # paires inversées\n",
        "                discordant += 1\n",
        "    return (concordant - discordant) / total_pairs if total_pairs > 0 else 1.0\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Autres métriques de distance\n",
        "# =============================\n",
        "\n",
        "def footrule_distance_norm(order: list[str], baseline: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Distance de Spearman Footrule (normalisée 0-1).\n",
        "    0 = identique au baseline ; 1 = totalement différent.\n",
        "    \"\"\"\n",
        "    n = len(order)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    r = {x: i for i, x in enumerate(order)}\n",
        "    b = {x: i for i, x in enumerate(baseline)}\n",
        "    dist = sum(abs(r[x] - b[x]) for x in order)\n",
        "    max_dist = (n*n) // 2  # distance max\n",
        "    return dist / max_dist if max_dist > 0 else 0.0\n",
        "\n",
        "def inversion_rate(order: list[str], baseline: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Proportion de paires inversées par rapport au baseline.\n",
        "    0 = même ordre, 1 = complètement inversé.\n",
        "    \"\"\"\n",
        "    n = len(order)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    pos_o = {x: i for i, x in enumerate(order)}\n",
        "    inv = 0\n",
        "    total = n * (n - 1) // 2\n",
        "    items = baseline\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            xi, xj = items[i], items[j]\n",
        "            if pos_o[xi] > pos_o[xj]:\n",
        "                inv += 1\n",
        "    return inv / total if total > 0 else 0.0\n",
        "\n",
        "def top_k_overlap(order_a: list[str], order_b: list[str], k: int) -> float:\n",
        "    \"\"\"\n",
        "    Taux de recouvrement entre les k premiers éléments de deux classements.\n",
        "    1.0 = les mêmes top-k.\n",
        "    \"\"\"\n",
        "    k = min(k, len(order_a), len(order_b))\n",
        "    if k <= 0:\n",
        "        return 0.0\n",
        "    return len(set(order_a[:k]) & set(order_b[:k])) / k\n",
        "\n",
        "\n",
        "# =============================\n",
        "# nDCG (Normalized Discounted Cumulative Gain)\n",
        "# =============================\n",
        "\n",
        "def make_graded_relevance_from_baseline(baseline: list[str]) -> dict[str, float]:\n",
        "    \"\"\"\n",
        "    Crée un score de pertinence basé sur la position dans le baseline.\n",
        "    Plus le nœud est haut dans le baseline, plus sa \"pertinence\" est forte.\n",
        "    \"\"\"\n",
        "    N = len(baseline)\n",
        "    return {node: float(N - i) for i, node in enumerate(baseline)}\n",
        "\n",
        "def ndcg_from_relevance(order: list[str], relevance: dict[str, float]) -> float:\n",
        "    \"\"\"\n",
        "    Mesure la qualité d’un classement par rapport à une pertinence de référence.\n",
        "    1.0 = classement idéal (comme le baseline).\n",
        "    \"\"\"\n",
        "    if not order:\n",
        "        return 0.0\n",
        "    dcg = 0.0\n",
        "    for i, n in enumerate(order):\n",
        "        rel = relevance.get(n, 0.0)\n",
        "        gain = (2**rel - 1.0)\n",
        "        dcg += gain / math.log2(i + 2.0)\n",
        "    ideal = sorted(order, key=lambda x: -relevance.get(x, 0.0))\n",
        "    idcg = 0.0\n",
        "    for i, n in enumerate(ideal):\n",
        "        rel = relevance.get(n, 0.0)\n",
        "        gain = (2**rel - 1.0)\n",
        "        idcg += gain / math.log2(i + 2.0)\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Algorithmes de classement\n",
        "# =============================\n",
        "\n",
        "def baseline_order(nodes: list[str]) -> list[str]:\n",
        "    \"\"\"\n",
        "    Baseline (pas un algorithme d’apprentissage) :\n",
        "    Tri simple par profondeur ↑, somme des chiffres ↑, puis ID ↑\n",
        "    \"\"\"\n",
        "    return sorted(nodes, key=lambda n: (depth_of(n), digit_sum(n), n))\n",
        "\n",
        "def pagerank_order(G: nx.DiGraph, nodes: list[str]):\n",
        "    \"\"\"\n",
        "    PageRank : calcule l’importance des nœuds selon le graphe.\n",
        "    Plus un nœud a de liens entrants importants, plus il est haut.\n",
        "    \"\"\"\n",
        "    start = time.perf_counter()\n",
        "    pr = nx.pagerank(G, alpha=0.85, max_iter=100, tol=1e-8)\n",
        "    elapsed_ms = (time.perf_counter() - start) * 1000\n",
        "    ordered = sorted(nodes, key=lambda n: (-pr.get(n, 0.0), n))\n",
        "    return ordered, pr, elapsed_ms\n",
        "\n",
        "def hits_order(G: nx.DiGraph, nodes: list[str]):\n",
        "    \"\"\"\n",
        "    HITS : calcule Autorité et Hub.\n",
        "    Ici on classe selon l'autorité (importance comme \"source d'information\").\n",
        "    \"\"\"\n",
        "    start = time.perf_counter()\n",
        "    try:\n",
        "        hubs, auth = nx.hits(G, max_iter=500, tol=1e-8, normalized=True)\n",
        "    except nx.exception.PowerIterationFailedConvergence:\n",
        "        hubs = {n: 1.0 for n in G.nodes()}\n",
        "        auth = {n: 1.0 for n in G.nodes()}\n",
        "    elapsed_ms = (time.perf_counter() - start) * 1000\n",
        "    ordered = sorted(nodes, key=lambda n: (-auth.get(n, 0.0), n))\n",
        "    return ordered, hubs, auth, elapsed_ms\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Affichage\n",
        "# =============================\n",
        "\n",
        "def print_algorithm_block(title: str, ordered_nodes: list[str], metrics: dict, extras: dict | None = None):\n",
        "    \"\"\"\n",
        "    Affiche les résultats d’un algorithme (ordre et métriques).\n",
        "    \"\"\"\n",
        "    print(f\"\\n— {title} —\")\n",
        "    print(\"Nodes to repass (ordered):\")\n",
        "    for n in ordered_nodes:\n",
        "        print(f\"  {n}\")\n",
        "    if extras:\n",
        "        for k, d in extras.items():\n",
        "            print(f\"  {k}:\")\n",
        "            for nk, nv in d.items():\n",
        "                if isinstance(nv, float):\n",
        "                    print(f\"    {nk}: {nv:.4f}\")\n",
        "                else:\n",
        "                    print(f\"    {nk}: {nv}\")\n",
        "    print(\"Metrics:\")\n",
        "    for k, v in metrics.items():\n",
        "        if isinstance(v, float):\n",
        "            print(f\"  {k}: {v:.4f}\")\n",
        "        else:\n",
        "            print(f\"  {k}: {v}\")\n",
        "\n",
        "\n",
        "# =============================\n",
        "# Main (programme principal)\n",
        "# =============================\n",
        "\n",
        "def main():\n",
        "    # Chargement des données\n",
        "    with open(\"graph.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        graph_adj = json.load(f)\n",
        "    with open(\"students.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        students = json.load(f)\n",
        "\n",
        "    # Saisie de l’étudiant\n",
        "    student_id = input(\"Entrez l'ID de l'étudiant : \").strip()\n",
        "    student = next((s for s in students if s[\"id\"] == student_id), None)\n",
        "    if not student:\n",
        "        print(\"Aucun étudiant trouvé avec cet ID.\")\n",
        "        return\n",
        "\n",
        "    # Construction du graphe\n",
        "    G = build_graph_from_adj(graph_adj)\n",
        "\n",
        "    # Nœuds à repasser = prérequis des modules échoués\n",
        "    failed = student.get(\"failed_nodes\", [])\n",
        "    candidates = immediate_prereqs(graph_adj, failed)\n",
        "\n",
        "    print(f\"\\nNœuds à refaire pour {student['name']} (ID: {student['id']}, Class: {student['class']}):\")\n",
        "    if not candidates:\n",
        "        print(\"  (Aucun prérequis direct trouvé.)\")\n",
        "        return\n",
        "\n",
        "    # ---------- Baseline ----------\n",
        "    t0 = time.perf_counter()\n",
        "    base_order = baseline_order(candidates)\n",
        "    base_time_ms = (time.perf_counter() - t0) * 1000\n",
        "\n",
        "    # Pertinence graduée dérivée du baseline\n",
        "    graded_rel = make_graded_relevance_from_baseline(base_order)\n",
        "\n",
        "    base_metrics = {\n",
        "        \"runtime_ms\": base_time_ms,\n",
        "        \"note\": \"Baseline is a simple deterministic ordering (not a learning algorithm).\",\n",
        "        \"ndcg_graded\": ndcg_from_relevance(base_order, graded_rel),  # toujours 1.0\n",
        "        \"footrule_norm_vs_self\": 0.0,\n",
        "        \"inversion_rate_vs_self\": 0.0,\n",
        "        \"top3_overlap_vs_self\": 1.0,\n",
        "        \"top5_overlap_vs_self\": 1.0\n",
        "    }\n",
        "    print_algorithm_block(\"Baseline (depth ↑, digit_sum ↑)\", base_order, base_metrics)\n",
        "\n",
        "    # ---------- PageRank ----------\n",
        "    pr_order, pr_scores, pr_time_ms = pagerank_order(G, candidates)\n",
        "    pr_metrics = {\n",
        "        \"runtime_ms\": pr_time_ms,\n",
        "        \"spearman_vs_baseline\": spearman(base_order, pr_order),\n",
        "        \"kendall_tau_vs_baseline\": kendall_tau(base_order, pr_order),\n",
        "        \"footrule_norm_vs_baseline\": footrule_distance_norm(pr_order, base_order),\n",
        "        \"inversion_rate_vs_baseline\": inversion_rate(pr_order, base_order),\n",
        "        \"top3_overlap_vs_baseline\": top_k_overlap(pr_order, base_order, k=3),\n",
        "        \"top5_overlap_vs_baseline\": top_k_overlap(pr_order, base_order, k=5),\n",
        "        \"ndcg_graded_vs_baseline\": ndcg_from_relevance(pr_order, graded_rel),\n",
        "    }\n",
        "    pr_extras = {\n",
        "        \"PageRank scores (candidates)\": {n: pr_scores.get(n, 0.0) for n in pr_order}\n",
        "    }\n",
        "    print_algorithm_block(\"PageRank\", pr_order, pr_metrics, extras=pr_extras)\n",
        "\n",
        "    # ---------- HITS ----------\n",
        "    hits_ordered, hubs, auth, hits_time_ms = hits_order(G, candidates)\n",
        "    hits_metrics = {\n",
        "        \"runtime_ms\": hits_time_ms,\n",
        "        \"spearman_vs_baseline\": spearman(base_order, hits_ordered),\n",
        "        \"kendall_tau_vs_baseline\": kendall_tau(base_order, hits_ordered),\n",
        "        \"footrule_norm_vs_baseline\": footrule_distance_norm(hits_ordered, base_order),\n",
        "        \"inversion_rate_vs_baseline\": inversion_rate(hits_ordered, base_order),\n",
        "        \"top3_overlap_vs_baseline\": top_k_overlap(hits_ordered, base_order, k=3),\n",
        "        \"top5_overlap_vs_baseline\": top_k_overlap(hits_ordered, base_order, k=5),\n",
        "        \"ndcg_graded_vs_baseline\": ndcg_from_relevance(hits_ordered, graded_rel),\n",
        "    }\n",
        "    hits_extras = {\n",
        "        \"HITS authority (candidates)\": {n: auth.get(n, 0.0) for n in hits_ordered}\n",
        "    }\n",
        "    print_algorithm_block(\"HITS (Authority)\", hits_ordered, hits_metrics, extras=hits_extras)\n",
        "\n",
        "    # ---------- Comparaison globale ----------\n",
        "    print(\"\\n=== Agreement Between Algorithms (info) ===\")\n",
        "    print(f\"Baseline vs PageRank: Spearman={spearman(base_order, pr_order):.3f}  KendallTau={kendall_tau(base_order, pr_order):.3f}\")\n",
        "    print(f\"Baseline vs HITS:     Spearman={spearman(base_order, hits_ordered):.3f}  KendallTau={kendall_tau(base_order, hits_ordered):.3f}\")\n",
        "    print(f\"PageRank vs HITS:     Spearman={spearman(pr_order, hits_ordered):.3f}    KendallTau={kendall_tau(pr_order, hits_ordered):.3f}\")\n",
        "\n",
        "    # ---------- Recommandation finale ----------\n",
        "    # Choix du meilleur algorithme (baseline exclue) basé sur nDCG puis Spearman puis vitesse\n",
        "    algo_scores = {\n",
        "        \"PageRank\": (pr_metrics[\"ndcg_graded_vs_baseline\"], pr_metrics[\"spearman_vs_baseline\"], -pr_time_ms),\n",
        "        \"HITS\":     (hits_metrics[\"ndcg_graded_vs_baseline\"], hits_metrics[\"spearman_vs_baseline\"], -hits_time_ms),\n",
        "    }\n",
        "    best_algo = max(algo_scores.items(), key=lambda kv: kv[1])[0]\n",
        "    print(f\"\\n>> Recommendation: {best_algo} (Baseline excluded; ranked by nDCG, then Spearman, then speed)\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Scgnyjvlt9",
        "outputId": "672131c9-e52d-490f-acef-e3ac8ddffb3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entrez l'ID de l'étudiant : 003\n",
            "\n",
            "Nœuds à refaire pour Mohamed Salah (ID: 003, Class: 1):\n",
            "\n",
            "— Baseline (depth ↑, digit_sum ↑) —\n",
            "Nodes to repass (ordered):\n",
            "  1.5\n",
            "  2.1\n",
            "  2.2\n",
            "  4.1\n",
            "  5.1\n",
            "  6.2\n",
            "Metrics:\n",
            "  runtime_ms: 0.0318\n",
            "  note: Baseline is a simple deterministic ordering (not a learning algorithm).\n",
            "  ndcg_graded: 1.0000\n",
            "  footrule_norm_vs_self: 0.0000\n",
            "  inversion_rate_vs_self: 0.0000\n",
            "  top3_overlap_vs_self: 1.0000\n",
            "  top5_overlap_vs_self: 1.0000\n",
            "\n",
            "— PageRank —\n",
            "Nodes to repass (ordered):\n",
            "  6.2\n",
            "  1.5\n",
            "  5.1\n",
            "  2.1\n",
            "  2.2\n",
            "  4.1\n",
            "  PageRank scores (candidates):\n",
            "    6.2: 0.0304\n",
            "    1.5: 0.0149\n",
            "    5.1: 0.0148\n",
            "    2.1: 0.0146\n",
            "    2.2: 0.0146\n",
            "    4.1: 0.0146\n",
            "Metrics:\n",
            "  runtime_ms: 2.4093\n",
            "  spearman_vs_baseline: -0.2000\n",
            "  kendall_tau_vs_baseline: -0.0667\n",
            "  footrule_norm_vs_baseline: 0.7778\n",
            "  inversion_rate_vs_baseline: 0.5333\n",
            "  top3_overlap_vs_baseline: 0.3333\n",
            "  top5_overlap_vs_baseline: 0.8000\n",
            "  ndcg_graded_vs_baseline: 0.6755\n",
            "\n",
            "— HITS (Authority) —\n",
            "Nodes to repass (ordered):\n",
            "  5.1\n",
            "  6.2\n",
            "  1.5\n",
            "  2.1\n",
            "  2.2\n",
            "  4.1\n",
            "  HITS authority (candidates):\n",
            "    5.1: 0.0970\n",
            "    6.2: 0.0000\n",
            "    1.5: 0.0000\n",
            "    2.1: 0.0000\n",
            "    2.2: 0.0000\n",
            "    4.1: 0.0000\n",
            "Metrics:\n",
            "  runtime_ms: 2.3557\n",
            "  spearman_vs_baseline: -0.3714\n",
            "  kendall_tau_vs_baseline: -0.0667\n",
            "  footrule_norm_vs_baseline: 0.8889\n",
            "  inversion_rate_vs_baseline: 0.5333\n",
            "  top3_overlap_vs_baseline: 0.3333\n",
            "  top5_overlap_vs_baseline: 0.8000\n",
            "  ndcg_graded_vs_baseline: 0.6003\n",
            "\n",
            "=== Agreement Between Algorithms (info) ===\n",
            "Baseline vs PageRank: Spearman=-0.200  KendallTau=-0.067\n",
            "Baseline vs HITS:     Spearman=-0.371  KendallTau=-0.067\n",
            "PageRank vs HITS:     Spearman=0.829    KendallTau=0.733\n",
            "\n",
            ">> Recommendation: PageRank (Baseline excluded; ranked by nDCG, then Spearman, then speed)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Les commentaires</h1>"
      ],
      "metadata": {
        "id": "s06nbpO3Bvo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# === 1. Chargement du graphe et identification des nœuds à repasser ===\n",
        "student_id = input(\"Entrez l'ID de l'étudiant : \")\n",
        "# Ce bloc demande l'ID de l'étudiant (ex: 003) pour cibler un étudiant précis.\n",
        "# Ensuite, le programme cherche les nœuds (modules) que l’étudiant doit repasser,\n",
        "# c’est-à-dire les prérequis immédiats des modules échoués.\n",
        "\n",
        "# === 2. Baseline (ordre déterministe) ===\n",
        "baseline_nodes = sort_by_depth_and_digit_sum(candidates)\n",
        "# Ici, on applique la baseline : ce n'est pas un algorithme d'apprentissage,\n",
        "# juste un tri fixe selon deux critères : profondeur du nœud (depth ↑)\n",
        "# puis somme des chiffres du nœud (digit_sum ↑).\n",
        "\n",
        "print(\"Nodes to repass (ordered):\", baseline_nodes)\n",
        "# Résultat observé :\n",
        "#  1.5 → 2.1 → 2.2 → 4.1 → 5.1 → 6.2\n",
        "# Les métriques internes (ndcg_graded, footrule_norm_vs_self, etc.)\n",
        "# sont toutes parfaites (1.0 ou 0.0) car on compare la baseline avec elle-même.\n",
        "# Temps d’exécution : ~0.03 ms (quasi instantané, simple tri).\n",
        "\n",
        "# === 3. PageRank ===\n",
        "pagerank_scores = nx.pagerank(G)\n",
        "pagerank_nodes = rank_candidates_by_score(candidates, pagerank_scores)\n",
        "# PageRank calcule l’importance d’un nœud selon :\n",
        "# - le nombre de liens entrants\n",
        "# - et l’importance des nœuds qui pointent vers lui\n",
        "# Plus un nœud reçoit des liens de nœuds eux-mêmes importants,\n",
        "# plus son score est élevé.\n",
        "\n",
        "print(\"Nodes to repass (ordered):\", pagerank_nodes)\n",
        "# Résultat observé :\n",
        "#  6.2 → 1.5 → 5.1 → 2.1 → 2.2 → 4.1\n",
        "# Le nœud 6.2 est le plus critique (score 0.0304).\n",
        "# Corrélation Spearman avec la baseline : -0.200 (classement assez inversé).\n",
        "# nDCG : 0.6755 → assez différent de la baseline mais conserve de bons candidats.\n",
        "# Overlap Top3 : 0.3333 → un seul nœud du Top3 baseline est commun.\n",
        "# Temps d’exécution : ~2.4 ms (plus long car calcul itératif).\n",
        "\n",
        "# === 4. HITS (Authority) ===\n",
        "hits_hubs, hits_authorities = nx.hits(G)\n",
        "hits_nodes = rank_candidates_by_score(candidates, hits_authorities)\n",
        "# HITS calcule deux scores par nœud :\n",
        "#  - Authority : importance en tant que \"référence\" (cité par beaucoup d’autres).\n",
        "#  - Hub : capacité à pointer vers d’autres nœuds importants.\n",
        "# Ici on se base sur l’autorité.\n",
        "\n",
        "print(\"Nodes to repass (ordered):\", hits_nodes)\n",
        "# Résultat observé :\n",
        "#  5.1 → 6.2 → 1.5 → 2.1 → 2.2 → 4.1\n",
        "# Le nœud 5.1 a l’autorité la plus forte (0.0970).\n",
        "# Spearman vs baseline : -0.3714 (ordre encore plus inversé que PageRank).\n",
        "# nDCG : 0.6003 → qualité plus faible que PageRank.\n",
        "# Overlap Top5 : 0.8000 → presque tous les nœuds restent présents dans le Top5.\n",
        "# Temps d’exécution : ~2.35 ms (proche de PageRank).\n",
        "\n",
        "# === 5. Comparaison entre algorithmes ===\n",
        "# On calcule la similarité des classements avec plusieurs métriques :\n",
        "#  - Spearman : corrélation de rangs (1 = même ordre, -1 = ordre inverse).\n",
        "#  - Kendall Tau : taux d’inversions de paires.\n",
        "#  - Footrule : distance normalisée entre permutations.\n",
        "#  - Top3/Top5 overlap : proportion d’éléments communs aux meilleurs rangs.\n",
        "\n",
        "print(\"Baseline vs PageRank:\", compute_spearman(baseline_nodes, pagerank_nodes))\n",
        "print(\"Baseline vs HITS:\", compute_spearman(baseline_nodes, hits_nodes))\n",
        "print(\"PageRank vs HITS:\", compute_spearman(pagerank_nodes, hits_nodes))\n",
        "# Résultats observés :\n",
        "#  - Baseline vs PageRank : Spearman = -0.200 (divergence modérée)\n",
        "#  - Baseline vs HITS : Spearman = -0.371 (divergence plus forte)\n",
        "#  - PageRank vs HITS : Spearman = 0.829 (forte similarité entre eux)\n",
        "\n",
        "# === 6. Recommandation finale ===\n",
        "best_algo = choose_best_algorithm([pagerank_metrics, hits_metrics])\n",
        "# La baseline est volontairement exclue car ce n’est pas un algorithme d’apprentissage.\n",
        "# Le choix se fait d’abord sur le nDCG, puis Spearman, puis la vitesse.\n",
        "# PageRank est recommandé :\n",
        "# - nDCG = 0.6755 (le plus élevé des vrais algorithmes)\n",
        "# - Accord fort avec HITS (Spearman 0.829)\n",
        "# - Temps d’exécution compétitif.\n",
        "print(\">> Recommendation:\", best_algo)"
      ],
      "metadata": {
        "id": "FzFWka-sBf5c"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}